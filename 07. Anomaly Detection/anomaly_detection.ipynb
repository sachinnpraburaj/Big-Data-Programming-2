{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import operator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('weather map').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AnomalyDetection():\n",
    "\n",
    "    def readData(self, filename):\n",
    "        self.rawDF = spark.read.parquet(filename).cache()\n",
    "\n",
    "    def readToyData(self):\n",
    "        data = [(0, [\"http\", \"udt\", 0.4]),(1, [\"http\", \"udf\", 0.5]),(2, [\"http\", \"tcp\", 0.5]),(3, [\"ftp\", \"icmp\", 0.1]),(4, [\"http\", \"tcp\", 0.4])]\n",
    "        schema = [\"id\", \"rawFeatures\"]\n",
    "        self.rawDF = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "    def cat2Num(self, df, indices):\n",
    "        def onehotencode(indexed,max_val):\n",
    "            onehot = list()\n",
    "            for i in range(int(max_val)):\n",
    "                onehot.append([1.0 if x == i else 0.0 for x in range(int(max_val))])\n",
    "            return onehot[int(indexed)]\n",
    "        \n",
    "        def delete_cat(features,indices):\n",
    "            indices.sort(reverse = True)\n",
    "            for index in indices:\n",
    "                del features[index]\n",
    "            features = [float(f) for f in features]\n",
    "            return features\n",
    "        \n",
    "        tocombine = list()\n",
    "        for index in indices:\n",
    "            df = df.withColumn(\"index_\"+str(index), df[\"rawFeatures\"][index])\n",
    "            indexer = StringIndexer(inputCol=\"index_\"+str(index), outputCol=\"newIndex_\"+str(index))\n",
    "            index_model = indexer.fit(df)\n",
    "            df = index_model.transform(df)\n",
    "            max_val = df.select([\"newIndex_\"+str(index)]).groupBy().max().collect()\n",
    "            max_val = max_val[0][0]+1\n",
    "            \n",
    "            onehotudf = udf(lambda x: onehotencode(x,max_val), ArrayType(FloatType()))\n",
    "            df = df.withColumn(\"onehot_\"+str(index), onehotudf(\"newIndex_\"+str(index)))\n",
    "            tocombine.append(\"onehot_\"+str(index))\n",
    "            \n",
    "        deludf = udf(lambda x: delete_cat(x,indices), ArrayType(FloatType()))\n",
    "        df = df.withColumn(\"without_cat\",deludf(\"rawFeatures\"))\n",
    "        tocombine.append(\"without_cat\")\n",
    "        \n",
    "        mergeCols = udf(lambda x, y: x + y, ArrayType(FloatType()))\n",
    "        for i in range(1,len(tocombine)):\n",
    "            if i == 1:\n",
    "                df = df.withColumn(\"combined_\"+str(i), mergeCols(col(tocombine[i-1]), col(tocombine[i])))\n",
    "            else:\n",
    "                df = df.withColumn(\"combined_\"+str(i), mergeCols(col(\"combined_\"+str(i-1)), col(tocombine[i])))\n",
    "        vectorudf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "        df = df.withColumn(\"features\", vectorudf(col(\"combined_\"+str(len(tocombine)-1)))).select(\"id\",\"rawfeatures\",\"features\")\n",
    "        return df\n",
    "\n",
    "    def addScore(self, df):\n",
    "        cluster_size = df.groupBy(\"prediction\").count()\n",
    "        N_max = cluster_size.groupBy().max().collect()[0][0]\n",
    "        N_min = cluster_size.groupBy().min().collect()[0][0]\n",
    "        score = cluster_size.withColumn(\"score\", (N_max - col(\"count\")) / (N_max - N_min))\n",
    "        df = df.join(score,\"prediction\").select(\"id\",\"rawFeatures\",\"features\",\"prediction\",\"score\")\n",
    "        return df\n",
    "\n",
    "    def detect(self, k, t):\n",
    "        \n",
    "        print (\"DF1:\")\n",
    "        #Encoding categorical features using one-hot.\n",
    "        df1 = self.cat2Num(self.rawDF, [0, 1]).cache()\n",
    "        df1.show()\n",
    "        \n",
    "        print (\"DF2:\")\n",
    "        print (\"training Kmeans Model...\")\n",
    "        #Clustering points using KMeans\n",
    "        features = df1.select(\"features\").rdd.map(lambda row: row[0]).cache()\n",
    "        model = KMeans.train(features, k, maxIterations=40, runs=10, initializationMode=\"random\", seed=20)\n",
    "        \n",
    "        print (\"making predictions...\")\n",
    "        #Adding the prediction column to df1\n",
    "        modelBC = spark.sparkContext.broadcast(model)\n",
    "        predictUDF = udf(lambda x: modelBC.value.predict(x), StringType())\n",
    "        \n",
    "        print (\"calculating score...\")\n",
    "        df2 = df1.withColumn(\"prediction\", predictUDF(df1.features)).cache()\n",
    "        df2.show()\n",
    "        \n",
    "        print (\"DF3:\")\n",
    "        #Adding the score column to df2; The higher the score, the more likely it is an anomaly\n",
    "        df3 = self.addScore(df2).cache()\n",
    "        df3.show()\n",
    "        \n",
    "        return df3.where(df3.score > t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF1:\n",
      "+-----+--------------------+--------------------+\n",
      "|   id|         rawfeatures|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|44263|[udp, SF, -0.1585...|[0.0,1.0,0.0,1.0,...|\n",
      "|44264|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44265|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44266|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44267|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44268|[udp, SF, -0.1585...|[0.0,1.0,0.0,1.0,...|\n",
      "|44269|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44270|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44271|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44272|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44273|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44274|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44275|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44276|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44277|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44278|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44279|[udp, SF, -0.1585...|[0.0,1.0,0.0,1.0,...|\n",
      "|44280|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44281|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "|44282|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DF2:\n",
      "training Kmeans Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachin199/spark/spark-2.3.1-bin-hadoop2.7/python/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions...\n",
      "calculating score...\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|   id|         rawfeatures|            features|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|44263|[udp, SF, -0.1585...|[0.0,1.0,0.0,1.0,...|         6|\n",
      "|44264|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         4|\n",
      "|44265|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         5|\n",
      "|44266|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         4|\n",
      "|44267|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         5|\n",
      "|44268|[udp, SF, -0.1585...|[0.0,1.0,0.0,1.0,...|         6|\n",
      "|44269|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|\n",
      "|44270|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         5|\n",
      "|44271|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         2|\n",
      "|44272|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         5|\n",
      "|44273|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         5|\n",
      "|44274|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         2|\n",
      "|44275|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         5|\n",
      "|44276|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         2|\n",
      "|44277|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         5|\n",
      "|44278|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         2|\n",
      "|44279|[udp, SF, -0.1585...|[0.0,1.0,0.0,1.0,...|         6|\n",
      "|44280|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         2|\n",
      "|44281|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|\n",
      "|44282|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         5|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DF3:\n",
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "|   id|         rawFeatures|            features|prediction|             score|\n",
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "|44269|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44281|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44305|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44307|[tcp, SF, -0.1527...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44309|[tcp, SF, -0.1556...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44312|[tcp, SF, -0.1556...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44318|[tcp, SF, -0.1578...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44319|[tcp, SF, -0.1578...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44341|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44363|[tcp, SF, -0.1578...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44427|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44439|[tcp, SF, -0.1439...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44443|[tcp, SF, -0.1578...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44449|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44451|[tcp, SF, -0.1578...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44460|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44461|[tcp, SF, -0.1563...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44480|[udp, SF, -0.1585...|[0.0,1.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44487|[tcp, SF, -0.1585...|[1.0,0.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "|44498|[udp, SF, -0.1585...|[0.0,1.0,0.0,1.0,...|         7|0.8061141026514893|\n",
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "number of anomalies ::  1436\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|   id|         rawFeatures|            features|prediction|score|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|44511|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|44537|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|44615|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|44623|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|44781|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|44822|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|44884|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|44896|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|44958|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45201|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45258|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45345|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45453|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45504|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45519|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45520|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45541|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45697|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45756|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "|45915|[icmp, SF, -0.158...|[0.0,0.0,1.0,1.0,...|         0|  1.0|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ad = AnomalyDetection()\n",
    "    ad.readToyData()\n",
    "    ad.readData('data/logs-features-sample')\n",
    "    anomalies = ad.detect(8, 0.97)\n",
    "    print (\"number of anomalies :: \",anomalies.count())\n",
    "    anomalies.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
